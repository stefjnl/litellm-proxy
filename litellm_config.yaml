# LiteLLM Proxy Configuration
# Shared proxy for local development - used by multiple Next.js apps
# Docs: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # Qwen 3.5 (default)
  - model_name: qwen3.5
    litellm_params:
      model: openai/qwen/qwen3.5-397b-a17b
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

  # GLM-5
  - model_name: glm-5
    litellm_params:
      model: openai/zai-org/glm-5
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

  # MiniMax M2.5
  - model_name: minimax-m2.5
    litellm_params:
      model: openai/minimax/minimax-m2.5-official
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

  # Kimi K2.5
  - model_name: kimi-k2.5
    litellm_params:
      model: openai/moonshotai/kimi-k2.5
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY
  
# Qwen3 Embedding 4B
  - model_name: qwen3-embedding
    litellm_params:
      model: openai/Qwen/Qwen3-Embedding-4B
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY
      encoding_format: float

litellm_settings:
  set_verbose: false
  drop_params: true
  default_model: qwen3.5

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  allow_requests_on_db_unavailable: true
