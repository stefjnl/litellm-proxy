# LiteLLM Proxy Configuration
# Shared proxy for local development - used by multiple Next.js apps
# Docs: https://docs.litellm.ai/docs/proxy/configs

model_list:
  # GLM-4.7
  - model_name: glm-4.7
    litellm_params:
      model: openai/zai-org/glm-4.7
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

  # MiniMax M2.1
  - model_name: minimax-m2.1
    litellm_params:
      model: openai/minimax/minimax-m2.1
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

  # Kimi K2.5
  - model_name: kimi-k2.5
    litellm_params:
      model: openai/moonshotai/kimi-k2.5
      api_base: https://nano-gpt.com/api/v1
      api_key: os.environ/NANOGPT_API_KEY

litellm_settings:
  set_verbose: false
  drop_params: true

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  allow_requests_on_db_unavailable: true
